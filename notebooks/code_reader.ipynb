{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\shrirams\\Documents\\LLMTools\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "from openai import OpenAI\n",
                "\n",
                "import os\n",
                "from typing import List, Union\n",
                "from pydantic import BaseModel\n",
                "from git import Repo, GitCommandError\n",
                "\n",
                "sys.path.append(\"C:\\\\Users\\\\SHIRAM\\\\PycharmProjects\\\\LLMStuff\\\\code\")\n",
                "from utils import str_to_path, get_message_obj\n",
                "from utils import init_chat_conversation, convert_history_to_convo\n",
                "\n",
                "from typing import List\n",
                "from blocks import (\n",
                "    get_repo_path_textbox,\n",
                "    get_max_tokens_slider,\n",
                "    get_model_names_dropdown,\n",
                "    get_prompt_options,\n",
                ")\n",
                "\n",
                "ROOT_PATH = \"C:\\\\Users\\\\SHIRAM\"\n",
                "root_path = str_to_path(path=ROOT_PATH)\n",
                "rep_path = str_to_path(path=f\"{ROOT_PATH}\\\\Documents\\\\markinghelperAI\")\n",
                "\n",
                "\n",
                "def get_chosen_files(repo_path: str) -> List[str]:\n",
                "    return gr.FileExplorer(\n",
                "        label=\"Repository Path\",\n",
                "        glob=\"**/*.py\",\n",
                "        file_count=\"multiple\",\n",
                "        root=repo_path,\n",
                "        show_label=False,\n",
                "    )\n",
                "\n",
                "\n",
                "def select_chat_window():\n",
                "    return gr.Tabs(selected=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "repo = Repo(root_path / \"PycharmProjects\" / \"LLMStuff\")\n",
                "\n",
                "class GitFileDiff(BaseModel):\n",
                "    filepath: Union[str, os.PathLike]\n",
                "    diff: str\n",
                "\n",
                "class AllGitFileDiffs(BaseModel):\n",
                "    diffs: List[GitFileDiff]\n",
                "\n",
                "def get_git_diffs(git_dir: Union[str, os.PathLike]) -> AllGitFileDiffs:\n",
                "    try:\n",
                "        repo = Repo(git_dir)\n",
                "    except GitCommandError:\n",
                "        raise ValueError(\"Invalid Git repository path\")\n",
                "\n",
                "    diffs = []\n",
                "    staged_files = [item.a_path for item in repo.index.diff(\"HEAD\")]\n",
                "\n",
                "    for file in staged_files:\n",
                "        try:\n",
                "            diff = repo.git.diff(\"HEAD\", file)\n",
                "            diffs.append(GitFileDiff(filepath=file, diff=diff))\n",
                "        except GitCommandError:\n",
                "            pass \n",
                "    \n",
                "    all_diffs = AllGitFileDiffs(diffs=diffs)\n",
                "    return all_diffs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "diffs = get_git_diffs(root_path / \"PycharmProjects\" / \"LLMStuff\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Gradio Frontend"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with gr.Blocks() as demo:\n",
                "    client = OpenAI()\n",
                "\n",
                "    # Init the file explorer\n",
                "    file_explorer = gr.FileExplorer(render=False)\n",
                "\n",
                "    # Define all the components here, render them later. Order matters for render.\n",
                "    repo_path = get_repo_path_textbox()\n",
                "    \n",
                "    # For the max tokens\n",
                "    max_tokens = get_max_tokens_slider()\n",
                "\n",
                "    # For the model name\n",
                "    model_names = get_model_names_dropdown()\n",
                "\n",
                "    # Another component, for checkboxes\n",
                "    prompt_options = get_prompt_options()\n",
                "\n",
                "    # You want to ensure that it's rendering after the button is clicked.\n",
                "    with gr.Tabs() as tabs:\n",
                "        with gr.TabItem(\"File Explorer\", id=0):\n",
                "            file_explorer.render()\n",
                "\n",
                "            # Time to layout the components, within one row\n",
                "            with gr.Row(equal_height=True, variant=\"panel\"):\n",
                "                with gr.Column(variant=\"panel\"):\n",
                "                    # Render repository path\n",
                "                    repo_path.render()\n",
                "\n",
                "                    # Trigger the file explorer\n",
                "                    choose_dir = gr.Button(\n",
                "                        value=\"Choose Directory\", variant=\"secondary\", scale=1\n",
                "                    )\n",
                "                    choose_dir.click(\n",
                "                        fn=get_chosen_files,\n",
                "                        inputs=[repo_path],\n",
                "                        outputs=[file_explorer],\n",
                "                    )\n",
                "\n",
                "                with gr.Column(variant=\"panel\"):\n",
                "                    with gr.Row(equal_height=True, variant=\"panel\"):\n",
                "                        prompt_options.render()\n",
                "                        model_names.render()\n",
                "\n",
                "                    with gr.Row(equal_height=True, variant=\"panel\"):\n",
                "                        max_tokens.render()\n",
                "\n",
                "                        goto_chat_tab = gr.Button(value=\"Start Chat\", render=True)\n",
                "                        goto_chat_tab.click(\n",
                "                            fn=select_chat_window,\n",
                "                            inputs=None,\n",
                "                            outputs=tabs,\n",
                "                        )\n",
                "\n",
                "        with gr.TabItem(\"Chat Window\", id=1):\n",
                "\n",
                "            def respond(msg, history, file_paths, model_name, max_tokens):\n",
                "                if len(history) == 0: \n",
                "                    convo = init_chat_conversation(file_paths=file_paths, code_only=True, user_question=msg)\n",
                "                else:\n",
                "                    convo = convert_history_to_convo(history=history)\n",
                "                    convo.messages.append(get_message_obj(msg, role='user'))\n",
                "\n",
                "                # Call the OpenAI API\n",
                "                response = client.chat.completions.create(\n",
                "                    messages=convo.model_dump()[\"messages\"],\n",
                "                    model=model_name,\n",
                "                    stream=True,\n",
                "                    max_tokens=max_tokens,\n",
                "                )\n",
                "\n",
                "                partial_message = \"\"\n",
                "                for chunk in response:\n",
                "                    if chunk.choices[0].delta.content:\n",
                "                        partial_message = partial_message + chunk.choices[0].delta.content\n",
                "                        yield partial_message\n",
                "\n",
                "                #return response.choices[0].message.content\n",
                "                \n",
                "            gr.ChatInterface(\n",
                "                respond,\n",
                "                additional_inputs=[file_explorer, model_names, max_tokens],\n",
                "            )\n",
                "\n",
                "\n",
                "demo.launch(height=1000)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
