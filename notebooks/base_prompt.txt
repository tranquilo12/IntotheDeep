```python
# app.py, run using `chainlit run app.py -w;`

import chainlit as cl
from dotenv import load_dotenv
from utils import init_convo
from oai_types import Conversation
from models import ModelNames

load_dotenv()


async def probe_llm(convo: Conversation, max_tokens: int = 500):
    # Instanciate empty message, for streaming into later
    streaming_response = cl.Message(content="")
    await streaming_response.send()

    stream = convo.call_llm(stream=True, max_tokens=max_tokens)
    complete_response = ""
    async for part in stream:
        await streaming_response.stream_token(part)
        complete_response += part

    # Update convo with assistant message, save it
    streaming_response.content = convo.messages_[-1].content.text

    # Cache this version of CONVO to memory
    cl.user_session.set("CONVO", convo)

    # Need an element that keeps track of the total tokens used.
    elements = [cl.Text(name="Token Count", content=str(convo.total_tokens))]
    streaming_response.elements = elements

    await streaming_response.update()


@cl.on_chat_start
async def on_chat_start():
    files = None
    while files == None:
        files = await cl.AskFileMessage(
            content="Please upload python files only.",
            accept={
                "text/plain": [".txt", ".py", ".env", ".html", ".css", ".js", ".csv"]
            },
            max_size_mb=10,
            timeout=240,
            max_files=4,
        ).send()

    all_code = []
    for py_f in files:
        with open(py_f.path, "r", encoding="utf-8") as f:
            code = f.read()
            formatted_code = f"### filename: {py_f.name} ###\n\n{code}\n\n###"
            all_code.append(formatted_code)

    first_msg: cl.Message = await cl.AskUserMessage(
        content="What do you want to do with these uploaded files?",
        type="assistant_message",
        timeout=60,
    ).send()

    if first_msg:
        CONVO = init_convo(
            context_code="\n\n".join(all_code),
            user_question=first_msg["output"],
            model_name=ModelNames.GPT_3_5_TURBO.value,
            code_only=False,
            efficient=False,
            explain=False,
        )

        # Didn't know what to call this step, "probing" the LLM
        # with a chat prompt seems the most logical.
        await probe_llm(convo=CONVO)


@cl.on_message
async def on_message(message: cl.Message):
    CONVO: Conversation = cl.user_session.get("CONVO")  # Get cached object
    CONVO.add_user_msg(msg=message.content)  # ingest user message

    # Probe llm
    await probe_llm(convo=CONVO)


if __name__ == "__main__":
    from chainlit.cli import run_chainlit

    run_chainlit(__file__)
```

```python
# oai_types.py, for declaring types for usage
import json
import base64
import tiktoken
import logging
import asyncio
from uuid import uuid4
from litellm import acompletion
from interpreter import execute_code_locally
from pydantic import BaseModel, Field, computed_field
from typing import List, Union, Dict, Literal, Optional, Callable, Any


#############################################
########## LLM related types ################
#############################################
class CodeExecutionContent(BaseModel):
    type: Literal["code_execution"] = "code_execution"
    code: str  # The executed code
    stdout: str  # Standard output from execution
    stderr: Optional[str] = None  # Standard error (if any)

    @computed_field
    @property
    def text(self) -> str:
        """Generates formatted Markdown text for code and output."""
        markdown_text = f"`python\n{self.code}\n`\n\n"

        if self.stdout:
            markdown_text += f"**Output:**\n`\n{self.stdout}\n`\n\n"

        if self.stderr:
            markdown_text += f"**Error:**\n`\n{self.stderr}\n`"

        return markdown_text

    def tokens(self, enc: tiktoken.Encoding) -> int:
        # Use the generated markdown text for token counting
        return len(enc.encode(self.text))


class TextContent(BaseModel):
    type: Literal["text"] = "text"
    text: str

    def tokens(self, enc: tiktoken.Encoding) -> int:
        return len(enc.encode(self.text))


class ImageContent(BaseModel):
    type: Literal["image_url"] = "image_url"
    image_url: Optional[str | Dict[str, str]]

    @classmethod
    def validate_image_path(cls, image_url):
        with open(image_url, "rb") as image_file:
            encoded_image = base64.b64encode(image_file.read()).decode("utf-8")
            image_url = {"url": f"data:image/jpeg;base64,{encoded_image}"}
        return image_url

    def tokens(self, enc: tiktoken.Encoding) -> int:
        return len(enc.encode(self.image_url))


class User(BaseModel):
    role: str = "user"
    content: TextContent

    def __init__(self, msg: str, **data):
        super().__init__(content=TextContent(text=msg), **data)


class Assistant(BaseModel):
    role: str = "assistant"
    content: Union[TextContent, CodeExecutionContent]
    name: Optional[str] = None
    tool_call_id: Optional[str] = None

    def __init__(
        self,
        msg: str = None,
        name: str = None,
        tool_call_id: str = None,
        code: str = None,
        stdout: str = None,
        stderr: str = None,
        **data,
    ):

        if msg is not None:  # If a regular text message is provided
            content = TextContent(text=msg)
        elif code is not None and stdout is not None:  # If code and output are provided
            content = CodeExecutionContent(code=code, stdout=stdout, stderr=stderr)
        else:
            raise ValueError("Either msg or (code and stdout) must be provided")

        super().__init__(content=content, name=name, tool_call_id=tool_call_id, **data)


class System(BaseModel):
    role: str = "system"
    content: TextContent

    def __init__(self, msg: str, **data):
        super().__init__(content=TextContent(text=msg), **data)


#############################################
############### For all tools ###############
#############################################


class Parameter(BaseModel):
    type: str
    description: str


class CodeBlockParameter(Parameter):
    type: Literal["string"] = "string"
    description: str = "The Python code to execute, enclosed in triple backticks (```)."


class StringParameter(Parameter):
    type: Literal["string"] = "string"
    description: str


class FunctionDefinition(BaseModel):
    name: str
    description: str
    parameters: Dict[str, Any] = Field(
        default_factory=lambda: {"type": "object", "properties": {}, "required": []},
        description="The parameters for the function call.",
    )


class Tool(BaseModel):
    type: Literal["function"] = "function"
    function: FunctionDefinition


class ExecCodeLocallyTool(Tool):
    function: FunctionDefinition = FunctionDefinition(
        name="execute_code_locally",
        description="Execute provided Python code locally and return the standard output and standard error.",
    )

    def add_parameter(self, name: str, parameter: Parameter, required: bool = False):
        """Adds a parameter to the function definition."""
        self.function.parameters["properties"][name] = parameter.model_dump(
            exclude_none=True
        )
        if required:
            self.function.parameters["required"].append(name)


#############################################
############### Convo Class #################
#############################################
class Conversation(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid4()))
    model_name: str = Field(default="gpt-3.5-turbo-0125")
    messages_: List[Union[User, Assistant, System]] = Field(default_factory=list)
    tools: List = Field(default=None)
    tool_choice: Dict = Field(default=None)
    pending_function_calls: Dict = Field(default={})
    pending_function_calls_queue: List = Field(default=[])
    active_function_call: str = Field(default="")

    @property
    def available_functions(self) -> Dict[str, Callable]:
        return {
            "execute_code_locally": execute_code_locally,
        }

    @property
    def token_encoding(self) -> tiktoken.Encoding:
        if self.model_name is not None:
            if "gpt" in self.model_name.lower():
                return tiktoken.encoding_for_model(self.model_name)

    @property
    def messages(self):
        return self.messages_

    @messages.setter
    def messages(self, value):
        raise AttributeError(
            "Cannot set messages directly. Use append() method instead."
        )

    @computed_field(return_type=int)
    @property
    def total_tokens(self):
        return sum(
            [msg.content.tokens(enc=self.token_encoding) for msg in self.messages_]
        )

    def __request_payload__(self, max_tokens: int, stream: bool):
        tool = ExecCodeLocallyTool()
        tool.add_parameter("code", CodeBlockParameter(), required=True)
        return {
            "model": self.model_name,
            "messages": self.to_dict(),
            "stream": stream,
            "max_tokens": max_tokens,
            "temperature": 0.0,
            "tools": [tool.model_dump(exclude_none=True)],
            "tool_choice": "auto",
        }

    def update_system_message(self, msg: str):
        self.messages_[0] = System(msg)

    def append(self, message: Union[User, Assistant, System]):
        self.messages_.append(message)

    def add_user_msg(self, msg: str):
        self.append(User(msg=msg))

    async def add_assistant_msg(
        self,
        msg: str = None,
        name: str = None,
        tool_call_id: str = None,
        code: str = None,
        stdout: str = None,
        stderr: str = None,
    ) -> None:
        """Asynchronously adds an assistant message to the conversation."""
        if msg is not None:
            assistant_message = Assistant(msg=msg, name=name, tool_call_id=tool_call_id)
        elif code is not None and stdout is not None:
            if isinstance(stdout, asyncio.Future) or isinstance(stderr, asyncio.Future):
                # If stdout or stderr are futures, await them
                stdout, stderr = await asyncio.gather(stdout, stderr)

            assistant_message = Assistant(
                code=code,
                stdout=stdout,
                stderr=stderr,
                name=name,
                tool_call_id=tool_call_id,
            )
        else:
            raise ValueError("Either msg or (code, stdout) must be provided")

        self.append(assistant_message)  # Add the assistant message to the conversation

    def del_message(self, index: int):
        if 0 <= index < len(self.messages_):
            del self.messages_[index]
        else:
            raise IndexError("Invalid message index.")

    def to_dict(self) -> List[Dict[str, str]]:
        return [
            {"role": message.role, "content": message.content.text}
            for message in self.messages_
        ]

    async def process_tool_calls(self, message):
        """
        Handles function calls received in the streamed LLM response, potentially in multiple chunks.

        This function now accumulates function call arguments and remembers the
        function name even if it's not repeated in subsequent chunks.

        Args:
            message: The message chunk from the LLM stream.
            max_tokens: Maximum tokens for any subsequent LLM calls after function execution.

        Yields:
            An empty string to maintain the stream flow.
        """

        function_call = message.get("function_call")
        if function_call:
            function_name = function_call.name
            function_args = function_call.arguments

            # Track the active function call
            if function_name:  # Update active function if provided
                self.active_function_call = function_name
            else:
                function_name = self.active_function_call  # Use the remembered name

            if function_name not in self.pending_function_calls:
                self.pending_function_calls[function_name] = ""

            self.pending_function_calls[function_name] += function_args

            # If the function call arguments are complete, move them to the queue
            if function_args.endswith("}"):  # Assuming JSON format for arguments
                args = self.pending_function_calls.pop(
                    function_name
                )  # Remove from pending_function_calls
                try:
                    parsed_args = json.loads(args)
                except json.JSONDecodeError as e:
                    print(f"Error parsing function arguments: {e}")
                    parsed_args = {}

                self.pending_function_calls_queue.append((function_name, parsed_args))

        yield ""  # Continue the stream without waiting for the function result

    async def call_llm(self, max_tokens: int, stream: bool):
        """
        Makes a call to the LLM, handling streaming and deferred function calls.

        Args:
            max_tokens: Maximum tokens for the LLM response.
            stream: Whether the response should be streamed.

        Yields:
            Tokens from the LLM response stream, or an empty string if waiting for function calls.
        """
        try:
            response = await acompletion(**self.__request_payload__(max_tokens, stream))
            if stream:
                async for chunk in response:
                    if token := chunk.choices[0].delta.content or "":
                        yield token
                    if "function_call" in chunk.choices[0].delta:
                        m_ = chunk.choices[0].delta
                        async for result in self.process_tool_calls(m_):
                            yield result

            else:
                first_response_message = response.choices[0].message
                yield first_response_message.content

            # Execute any pending function calls after the stream completes or after a non-streamed response
            for function_name, function_args in self.pending_function_calls_queue:
                function_to_call = self.available_functions[function_name]
                function_response = await function_to_call(
                    code=function_args.get("code")
                )  # Await asynmax_token function

                stdout, stderr = function_response
                await self.add_assistant_msg(
                    code=function_args.get("code"),
                    stdout=stdout,
                    stderr=stderr,
                    name=function_name,
                )

            # Clear pending function calls after execution
            self.pending_function_calls = {}
            self.pending_function_calls_queue = []

        except Exception as e:
            logging.exception("Error in call_llm: %s", str(e))
            raise
```python
# interpreter.py 
import json
import aiohttp
from typing import Tuple


class Interpreter:
    def __init__(self, endpoint: str):
        self.endpoint = endpoint

    async def run(self, code: str) -> Tuple[str, str]:
        async with aiohttp.ClientSession() as session:
            async with session.post(self.endpoint, json={"code": code}) as response:
                result = await response.text()
                result = json.loads(result)
                return result["stdout"], result["stderr"]


async def execute_code_locally(code: str) -> Tuple[str, str]:
    repl = Interpreter(endpoint="http://localhost:8888/execute")
    output, error = await repl.run(code)
    return output, error
```

I've provided you with some python code that's responsible for handling LLM interactions 
and helps me execute code locally with a local python interpreter. 

##### 
Ok, now think about the flow of the conversation. Think about the situations below: 
1. User uploads file, provides first comment and the response doesn't contain any function calling and is streaming.
2. User uploads file, provides first comment and the response doesn't contain any function calling and is NOT streaming  
3. User uploads file, provides first comment and the response contains function calling, and is streaming.
4. User uploads file, provides first comment and the response contains function calling, and is NOT streaming. 

Check for any inconsistencies in updating the main Conversation class when either of these flows takes place. 